This dataset contains the evaluation/benchmark for PuzzleEval.

PuzzleEval is a benchmark that evaluates LLMs capability to solve puzzle games such as Mastermind, Word Ladder... etc. This benchmark is designed to test the reasoning capabilities of many reasoning models, while avoiding the possibility of being trained on, as it is possible to generate virtually infinite amount of instances.

Currently, the dataset contains puzzles for:

- Mastermind: Given a secret code containing n-pegs and m-colors, as well as the clues needed to reach the answer, the model needs to be able to guess the secret code based on the clues. The puzzle generation algorithm ensure that there can be only 1 unique solution. This is done by first generating clues for the secret code, then pruning redundant clues (clues that does not change the possible number of secret codes) until 1 solution remain.
- Word ladder: Given a start word and a target word, one must find a chain of other words to link the two, in which two adjacent words (that is, words in successive steps) differ by one letter, note that there are many possible solutions to a word ladder puzzle, but the answer field only provides one of which. This repo provides a Python script to validate a word ladder
- Quintumble: Based on https://quintumble.com. Quintumble is a word puzzle game in which the player has to rearrange the letters within each column to make 5 words, one for each row. The puzzles are generated by taking 5 random words and arranging them into a grid, and shuffling the columns (except for the first column).
